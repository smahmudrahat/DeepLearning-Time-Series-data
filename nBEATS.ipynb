{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDvPWBrpniPM",
        "outputId": "5bbb4bdc-4039-4d7d-9471-56f2649a940b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "24GgkOglinkt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from pandas import read_csv\n",
        "from scipy import stats\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import trange, tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 192\n",
        "stride_size = 24\n",
        "target_window_size = 24\n",
        "history_size = 150"
      ],
      "metadata": {
        "id": "UOOVQUa9jHHS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_start = '2011-01-01 00:00:00'\n",
        "train_end = '2014-08-31 23:00:00'\n",
        "test_start = '2014-08-25 00:00:00' #need additional 7 days as given info\n",
        "test_end = '2014-09-07 23:00:00'\n",
        "\n",
        "name = 'LD2011_2014.txt'\n",
        "save_name = 'elect'\n",
        "save_path = os.path.join('data', save_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "csv_path = os.path.join(save_path, name)\n",
        "if not os.path.exists(csv_path):\n",
        "    zipurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip'\n",
        "    with urlopen(zipurl) as zipresp:\n",
        "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
        "            zfile.extractall(save_path)\n",
        "\n",
        "data_frame = pd.read_csv(csv_path, sep=\";\", index_col=0, parse_dates=True, decimal=',')\n",
        "data_frame = data_frame.resample('1H',label = 'left',closed = 'right').sum()[train_start:test_end]\n",
        "data_frame.fillna(0, inplace=True) # (32304, 370)"
      ],
      "metadata": {
        "id": "jVCHF3vwjLd8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the training data from the DataFrame within the specified date range.\n",
        "train_data = data_frame[train_start:train_end]\n",
        "\n",
        "# Select the testing data from the DataFrame within the specified date range.\n",
        "test_data = data_frame[test_start:test_end]\n",
        "\n",
        "# Import the MinMaxScaler from sklearn for data normalization.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler.\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data to learn the scaling parameters.\n",
        "scaler.fit(train_data)\n",
        "\n",
        "# Apply the scaler to the training data and create a DataFrame with the same indices and columns.\n",
        "train_target_df = pd.DataFrame(scaler.transform(train_data), index=train_data.index, columns=train_data.columns)\n",
        "\n",
        "# Apply the scaler to the testing data and create a DataFrame with the same indices and columns.\n",
        "test_target_df = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)\n",
        "\n",
        "# Convert the scaled training DataFrame to a NumPy array for further processing.\n",
        "train_data = train_target_df.values\n",
        "\n",
        "# Convert the scaled testing DataFrame to a NumPy array for further processing.\n",
        "test_data = test_target_df.values\n"
      ],
      "metadata": {
        "id": "Pc-rA4fKjM5F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for sampling time series data.\n",
        "class TimeseriesSampler:\n",
        "    # Initialize the sampler with parameters like size of input and output samples, limit for window sampling, and batch size.\n",
        "    def __init__(self,\n",
        "                 timeseries: np.ndarray,\n",
        "                 insample_size: int=window_size,\n",
        "                 outsample_size: int=target_window_size,\n",
        "                 window_sampling_limit: int=history_size * target_window_size,\n",
        "                 batch_size: int = 8):\n",
        "        # Store the input time series and other parameters.\n",
        "        self.timeseries = [ts for ts in timeseries]\n",
        "        self.window_sampling_limit = window_sampling_limit\n",
        "        self.batch_size = batch_size\n",
        "        self.insample_size = insample_size\n",
        "        self.outsample_size = outsample_size\n",
        "\n",
        "    # Define an iterator for generating batches of samples.\n",
        "    def __iter__(self):\n",
        "        # Continuously generate samples.\n",
        "        while True:\n",
        "            # Initialize arrays for input samples and their masks.\n",
        "            insample = np.zeros((self.batch_size, self.insample_size))\n",
        "            insample_mask = np.zeros((self.batch_size, self.insample_size))\n",
        "            # Initialize arrays for output samples and their masks.\n",
        "            outsample = np.zeros((self.batch_size, self.outsample_size))\n",
        "            outsample_mask = np.zeros((self.batch_size, self.outsample_size))\n",
        "            # Randomly select indices of time series to sample from.\n",
        "            sampled_ts_indices = np.random.randint(len(self.timeseries), size=self.batch_size)\n",
        "            for i, sampled_index in enumerate(sampled_ts_indices):\n",
        "                # Select the sampled time series.\n",
        "                sampled_timeseries = self.timeseries[sampled_index]\n",
        "                # Randomly choose a point to cut the time series for sampling.\n",
        "                cut_point = np.random.randint(low=max(1, len(sampled_timeseries) - self.window_sampling_limit),\n",
        "                                              high=len(sampled_timeseries),\n",
        "                                              size=1)[0]\n",
        "\n",
        "                # Extract the input sample from the time series.\n",
        "                insample_window = sampled_timeseries[max(0, cut_point - self.insample_size):cut_point]\n",
        "                insample[i, -len(insample_window):] = insample_window\n",
        "                insample_mask[i, -len(insample_window):] = 1.0\n",
        "                # Extract the output sample from the time series.\n",
        "                outsample_window = sampled_timeseries[cut_point:min(len(sampled_timeseries), cut_point + self.outsample_size)]\n",
        "                outsample[i, :len(outsample_window)] = outsample_window\n",
        "                outsample_mask[i, :len(outsample_window)] = 1.0\n",
        "            # Yield the generated input and output samples along with their masks.\n",
        "            yield insample, insample_mask, outsample, outsample_mask\n",
        "\n",
        "    # Method to get the last input sample window from each time series.\n",
        "    def last_insample_window(self):\n",
        "        # Initialize arrays for the last input sample and its mask.\n",
        "        insample = np.zeros((len(self.timeseries), self.insample_size))\n",
        "        insample_mask = np.zeros((len(self.timeseries), self.insample_size))\n",
        "        for i, ts in enumerate(self.timeseries):\n",
        "            # Extract the last input sample window from the time series.\n",
        "            ts_last_window = ts[-self.insample_size:]\n",
        "            insample[i, -len(ts):] = ts_last_window\n",
        "            insample_mask[i, -len(ts):] = 1.0\n",
        "        # Return the last input sample and its mask.\n",
        "        return insample, insample_mask\n"
      ],
      "metadata": {
        "id": "rXqJKVZHkzC4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a time series data loader for training.\n",
        "train_loader = TimeseriesSampler(timeseries=train_data.T)\n",
        "\n",
        "# Define a generic basis function module for the N-Beats model.\n",
        "class GenericBasis(nn.Module):\n",
        "    # Initialize the module with backcast and forecast sizes.\n",
        "    def __init__(self, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.backcast_size, self.forecast_size = backcast_size, forecast_size\n",
        "\n",
        "    # Forward pass splits the theta vector into backcast and forecast components.\n",
        "    def forward(self, theta):\n",
        "        return theta[:, :self.backcast_size], theta[:, -self.forecast_size:]\n",
        "\n",
        "# Define a single N-Beats block module.\n",
        "class NBeatsBlock(nn.Module):\n",
        "    # Initialize the block with specified sizes for layers and theta vector, and a basis function.\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 theta_size: int,\n",
        "                 basis_function: nn.Module,\n",
        "                 layers: int,\n",
        "                 layer_size: int):\n",
        "        super().__init__()\n",
        "        # Create a sequence of linear layers.\n",
        "        self.layers = nn.ModuleList([nn.Linear(in_features=input_size, out_features=layer_size)] +\n",
        "                                      [nn.Linear(in_features=layer_size, out_features=layer_size)\n",
        "                                       for _ in range(layers - 1)])\n",
        "        # Linear layer for generating basis parameters.\n",
        "        self.basis_parameters = nn.Linear(in_features=layer_size, out_features=theta_size)\n",
        "        # Assign the provided basis function.\n",
        "        self.basis_function = basis_function\n",
        "\n",
        "    # Define the forward pass for the N-Beats block.\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Input to the first layer.\n",
        "        block_input = x\n",
        "        # Pass input through each layer, applying ReLU activation function.\n",
        "        for layer in self.layers:\n",
        "            block_input = torch.relu(layer(block_input))\n",
        "        # Compute basis parameters.\n",
        "        basis_parameters = self.basis_parameters(block_input)\n",
        "        # Compute backcast and forecast using the basis function.\n",
        "        return self.basis_function(basis_parameters)\n",
        "\n",
        "# Define the overall N-Beats model.\n",
        "class NBeats(nn.Module):\n",
        "    # Initialize with a list of N-Beats blocks.\n",
        "    def __init__(self, blocks: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.blocks = blocks\n",
        "\n",
        "    # Define the forward pass for the N-Beats model.\n",
        "    def forward(self, x: torch.Tensor, input_mask: torch.Tensor) -> torch.Tensor:\n",
        "        # Initialize residuals and flip them for processing.\n",
        "        residuals = x.flip(dims=(1,))\n",
        "        # Flip the input mask for alignment with residuals.\n",
        "        input_mask = input_mask.flip(dims=(1,))\n",
        "        # Initialize forecast with the last value in the input.\n",
        "        forecast = x[:, -1:]\n",
        "        # Iterate through each block, updating residuals and forecast.\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            backcast, block_forecast = block(residuals)\n",
        "            # Update residuals by subtracting backcast, applying mask.\n",
        "            residuals = (residuals - backcast) * input_mask\n",
        "            # Update the forecast by adding block forecast.\n",
        "            forecast = forecast + block_forecast\n",
        "        return forecast\n",
        "\n",
        "# Define a function to convert numpy arrays to PyTorch tensors.\n",
        "def to_tensor(array: np.ndarray):\n",
        "    return torch.tensor(array, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "u-AeQ-kGlCDS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function for the model.\n",
        "def train(model, device=torch.device('cuda'), iterations=1000, num_epochs = 1, learning_rate = 1e-3):\n",
        "    # Initialize the optimizer with Adam algorithm and learning rate.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # List to store loss values after each epoch.\n",
        "    loss_summary = []\n",
        "    # Define the loss function as mean squared error.\n",
        "    loss_fn = F.mse_loss\n",
        "    # Create an iterator for the training data loader.\n",
        "    training_set = iter(train_loader)\n",
        "\n",
        "    # Loop over each epoch.\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set the model to training mode.\n",
        "        model.train()\n",
        "\n",
        "        # Initialize a progress bar for the number of iterations.\n",
        "        pbar = trange(iterations)\n",
        "        # Iterate over each batch in the training data.\n",
        "        for iteration in pbar:\n",
        "            # Extract and convert the input and target data to tensors.\n",
        "            x, x_mask, y, y_mask = map(to_tensor, next(training_set))\n",
        "            # Reset gradients to zero before starting backpropagation.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize loss to zero.\n",
        "            loss = torch.zeros(1, device=device, dtype=torch.float32)\n",
        "            # Perform a forward pass of the model and compute output.\n",
        "            out = model(x.to(device), x_mask.to(device))\n",
        "            # Compute the loss between output and actual target.\n",
        "            loss = loss_fn(out.float(), y.squeeze().to(device).float())\n",
        "\n",
        "            # Update the progress bar with current loss.\n",
        "            pbar.set_description(f\"Loss:{loss.item()}\")\n",
        "            # Perform backpropagation to compute gradients.\n",
        "            loss.backward()\n",
        "            # Update model parameters.\n",
        "            optimizer.step()\n",
        "\n",
        "        # Append the last loss value to the summary list.\n",
        "        loss_summary.append(loss.cpu().detach())\n",
        "\n",
        "    # Return the loss summary and the optimizer.\n",
        "    return loss_summary, optimizer\n",
        "\n",
        "# Define the evaluation function for the model.\n",
        "def evaluate(model, optimizer, device=torch.device('cuda')):\n",
        "    # List to store forecasted values.\n",
        "    forecasts = []\n",
        "    # Calculate the number of test windows.\n",
        "    test_windows = test_data.T.shape[1] // target_window_size\n",
        "\n",
        "    # Disable gradient calculations for evaluation.\n",
        "    with torch.no_grad():\n",
        "        # Set the model to evaluation mode.\n",
        "        model.eval()\n",
        "        # Iterate over each test window.\n",
        "        for i in trange(test_windows):\n",
        "            # Combine training and test data up to the current window.\n",
        "            window_input_set = np.concatenate([train_data.T, test_data.T[:, :i * target_window_size]], axis=1)\n",
        "            # Create a time series sampler for the combined data.\n",
        "            input_set = TimeseriesSampler(timeseries=window_input_set)\n",
        "            # Extract the last in-sample window.\n",
        "            x, x_mask = map(to_tensor, input_set.last_insample_window())\n",
        "            # Get model predictions for the current window.\n",
        "            window_forecast = model(x.to(device), x_mask.to(device)).cpu().detach().numpy()\n",
        "            # Append the forecast to the forecasts list.\n",
        "            forecasts = window_forecast if len(forecasts) == 0 else np.concatenate([forecasts, window_forecast], axis=1)\n",
        "\n",
        "    # Return the root mean square error between forecasts and actual test data.\n",
        "    return np.sqrt(np.mean((forecasts - test_data.T) ** 2))\n"
      ],
      "metadata": {
        "id": "PtSgbcKalSDC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBeats(nn.ModuleList([NBeatsBlock(input_size=window_size,\n",
        "                                           theta_size=window_size + target_window_size,\n",
        "                                           basis_function=GenericBasis(backcast_size=window_size,\n",
        "                                                                       forecast_size=target_window_size),\n",
        "                                           layers=4,\n",
        "                                           layer_size=512)\n",
        "                                   for _ in range(30)])).cuda()\n",
        "\n",
        "loss, optimizer = train(model, num_epochs=3)\n",
        "model_save_path = '/content/drive/MyDrive/Colab_Notebooks/ads_506/nbeats.pth'\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1nRF8ZlmkX-",
        "outputId": "cf963035-6cc6-4ea8-f6dd-9df00b7eea63"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss:0.0036280089989304543: 100%|██████████| 1000/1000 [00:51<00:00, 19.44it/s]\n",
            "Loss:0.004546383861452341: 100%|██████████| 1000/1000 [00:51<00:00, 19.32it/s]\n",
            "Loss:0.004569113254547119: 100%|██████████| 1000/1000 [00:52<00:00, 19.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tg_mthJmmhv",
        "outputId": "5cc22953-0d7f-4c4c-9ca6-60ddcba9f789"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 20.49it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06692914282696105"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}