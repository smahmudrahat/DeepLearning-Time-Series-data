{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPkOIVtBiC02",
        "outputId": "3981e508-7244-4f72-8275-0028fbefbfd0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gy4_ZjsoUyKp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from pandas import read_csv\n",
        "from scipy import stats\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import trange, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 192\n",
        "stride_size = 24\n",
        "target_window_size = 24\n",
        "num_covariates = 3\n",
        "train_start = '2011-01-01 00:00:00'\n",
        "train_end = '2014-08-31 23:00:00'\n",
        "test_start = '2014-08-25 00:00:00' #need additional 7 days as given info\n",
        "test_end = '2014-09-07 23:00:00'\n"
      ],
      "metadata": {
        "id": "NBvy4B1mVeMn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(data, covariates, data_start, train = True):\n",
        "    # Calculate the length of time series data.\n",
        "    time_len = data.shape[0]\n",
        "    # Determine input size based on window and stride sizes.\n",
        "    input_size = window_size - stride_size\n",
        "    # Calculate the number of windows per series based on the data length, input size, and target window size.\n",
        "    windows_per_series = np.full((num_series), (time_len - input_size - target_window_size) // stride_size)\n",
        "    # If training, adjust the number of windows per series based on the data start index.\n",
        "    if train: windows_per_series -= (data_start + stride_size - 1) // stride_size\n",
        "    # Calculate the total number of windows across all series.\n",
        "    total_windows = np.sum(windows_per_series)\n",
        "    # Initialize arrays for input data, labels, and additional inputs.\n",
        "    x_input = np.zeros((total_windows, window_size, 1 + num_covariates), dtype='float32')\n",
        "    label = np.zeros((total_windows, target_window_size, 1 + num_covariates), dtype='float32')\n",
        "    v_input = np.zeros((total_windows, 2), dtype='float32')\n",
        "    # Initialize a counter for tracking the current window.\n",
        "    count = 0\n",
        "    # Iterate over each time series to process data.\n",
        "    for series in trange(num_series):\n",
        "        # Iterate over each window in the current series.\n",
        "        for i in range(windows_per_series[series]):\n",
        "            # Calculate the start index of the window based on whether it's training or testing data.\n",
        "            if train:\n",
        "                window_start = stride_size * i + data_start[series]\n",
        "            else:\n",
        "                window_start = stride_size * i\n",
        "            # Calculate the end index of the input window and the target window.\n",
        "            window_end = window_start + window_size\n",
        "            target_window_end = window_end + target_window_size\n",
        "            # Assign data and covariates to the input array.\n",
        "            x_input[count, :, 0] = data[window_start:window_end, series]\n",
        "            x_input[count, :, 1:1 + num_covariates] = covariates[window_start:window_end, :]\n",
        "            # Assign target data and covariates to the label array.\n",
        "            label[count, :, 0] = data[window_end:target_window_end, series]\n",
        "            label[count, :, 1:1 + num_covariates] = covariates[window_end:target_window_end, :]\n",
        "            # Calculate the sum of non-zero elements in the current window.\n",
        "            nonzero_sum = (x_input[count, 1:input_size, 0] != 0).sum()\n",
        "            # Handle cases where the sum of non-zero elements is zero.\n",
        "            if nonzero_sum == 0:\n",
        "                v_input[count, 0] = 0\n",
        "            else:\n",
        "                # Calculate the scaling factor for normalization and apply it to the input and label data.\n",
        "                v_input[count, 0] = np.true_divide(x_input[count, :input_size, 0].sum(), nonzero_sum) + 1\n",
        "                x_input[count, :, 0] = x_input[count, :, 0] / v_input[count, 0]\n",
        "                label[count, :, 0] = label[count, :, 0] / v_input[count, 0]\n",
        "            # Increment the window count.\n",
        "            count += 1\n",
        "    # Return the prepared input data, additional input, and labels.\n",
        "    return x_input, v_input, label\n"
      ],
      "metadata": {
        "id": "B-EG2-sGVf0E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_covariates(times, num_covariates):\n",
        "    covariates = np.zeros((times.shape[0], num_covariates))\n",
        "    for i, input_time in enumerate(times):\n",
        "        covariates[i, 0] = input_time.weekday()\n",
        "        covariates[i, 1] = input_time.hour\n",
        "        covariates[i, 2] = input_time.month\n",
        "    return covariates[:, :num_covariates]"
      ],
      "metadata": {
        "id": "ihdQvgIYWUxq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'LD2011_2014.txt'\n",
        "save_name = 'elect'\n",
        "save_path = os.path.join('data', save_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "csv_path = os.path.join(save_path, name)\n",
        "if not os.path.exists(csv_path):\n",
        "    zipurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip'\n",
        "    with urlopen(zipurl) as zipresp:\n",
        "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
        "            zfile.extractall(save_path)\n",
        "\n",
        "data_frame = pd.read_csv(csv_path, sep=\";\", index_col=0, parse_dates=True, decimal=',')\n",
        "data_frame = data_frame.resample('1H',label = 'left',closed = 'right').sum()[train_start:test_end]\n",
        "data_frame.fillna(0, inplace=True) # (32304, 370)\n",
        "# generate covariates (has both train and test limits)\n",
        "covariates = gen_covariates(data_frame[train_start:test_end].index, num_covariates) # (32304, 3)"
      ],
      "metadata": {
        "id": "ytzdW1AMWWLz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MinMaxScaler from scikit-learn for data normalization.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Determine the number of unique values in each covariate.\n",
        "cov_dims = pd.DataFrame(covariates).nunique().tolist()\n",
        "\n",
        "# Select the training data from the data frame based on specified start and end indices.\n",
        "train_data = data_frame[train_start:train_end]\n",
        "\n",
        "# Select the test data from the data frame based on specified start and end indices.\n",
        "test_data = data_frame[test_start:test_end]\n",
        "\n",
        "# Initialize the MinMaxScaler for normalization.\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the training data to learn the scaling parameters.\n",
        "scaler.fit(train_data)\n",
        "\n",
        "# Transform and normalize the training data using the fitted scaler, preserving the index and column names.\n",
        "train_target_df = pd.DataFrame(scaler.transform(train_data), index=train_data.index, columns=train_data.columns)\n",
        "\n",
        "# Transform and normalize the test data using the same scaler, preserving the index and column names.\n",
        "test_target_df = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)\n",
        "\n",
        "# Convert the normalized training data DataFrame to a NumPy array for processing.\n",
        "train_data = train_target_df.values\n",
        "\n",
        "# Convert the normalized test data DataFrame to a NumPy array for processing.\n",
        "test_data = test_target_df.values\n",
        "\n",
        "# Find the first non-zero value in each time series of the training data.\n",
        "data_start = (train_data != 0).argmax(axis=0)\n",
        "\n",
        "# Store the total number of time points in the dataset.\n",
        "total_time = data_frame.shape[0] # 32304\n",
        "\n",
        "# Store the number of individual time series in the dataset.\n",
        "num_series = data_frame.shape[1] # 370\n",
        "\n",
        "# Prepare the training data using the prep_data function, providing train_data, covariates, and data_start.\n",
        "X_train, v_train, y_train = prep_data(train_data, covariates, data_start)\n",
        "\n",
        "# Prepare the test data using the prep_data function, specifying that it is test data (train=False).\n",
        "X_test, v_test, y_test = prep_data(test_data, covariates, data_start, train=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNR2V1BAWcq3",
        "outputId": "c4b0fb56-1b72-471b-c649-7086d6468444"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 370/370 [00:10<00:00, 36.43it/s]\n",
            "100%|██████████| 370/370 [00:00<00:00, 8454.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`class TrainDataset`: it defines a custom dataset class for training data compatible with PyTorch.\n",
        "\n",
        "`class TestDataset`: it defines a custom dataset class for test data compatible with PyTorch"
      ],
      "metadata": {
        "id": "wwMqZJHGYGUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        # Initialize dataset with data and labels.\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        # Store the number of samples in the dataset.\n",
        "        self.train_len = self.data.shape[0]\n",
        "\n",
        "    # Return the length of the dataset.\n",
        "    def __len__(self):\n",
        "        return self.train_len\n",
        "\n",
        "    # Define method to get a specific item from the dataset by index.\n",
        "    def __getitem__(self, index):\n",
        "        # Return a tuple of current time series sequence, current covariates, label sequence, and future covariates.\n",
        "        return (self.data[index, :, 0], self.data[index, :, 1:1+num_covariates], self.label[index, :, 0], self.label[index, :, 1:1+num_covariates])\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data, v, label):\n",
        "        # Initialize dataset with data, normalizing stats, and labels.\n",
        "        self.data = data\n",
        "        self.v = v\n",
        "        self.label = label\n",
        "        # Store the number of samples in the dataset.\n",
        "        self.test_len = self.data.shape[0]\n",
        "\n",
        "    # Return the length of the dataset.\n",
        "    def __len__(self):\n",
        "        return self.test_len\n",
        "\n",
        "    # Define method to get a specific item from the dataset by index.\n",
        "    def __getitem__(self, index):\n",
        "        # Return a tuple of current time series sequence, current covariates, normalizing stats, label sequence, and future covariates.\n",
        "        return (self.data[index, :, 0], self.data[index, :, 1:1+num_covariates], self.v[index], self.label[index, :, 0], self.label[index, :, 1:1+num_covariates])\n"
      ],
      "metadata": {
        "id": "gz8ed3s0XfMk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 8\n",
        "\n",
        "train_set = TrainDataset(X_train, y_train)\n",
        "test_set = TestDataset(X_test, v_test, y_test)\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_set, batch_size=len(test_set), sampler=RandomSampler(test_set))"
      ],
      "metadata": {
        "id": "Biv-Djcda9xt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for a residual block in a neural network.\n",
        "class ResidualBlock(nn.Module):\n",
        "    # Initialize the residual block with specified parameters.\n",
        "    def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # Store kernel size, dilation, and dropout function.\n",
        "        self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p)\n",
        "\n",
        "        # Define two 1D convolutional layers.\n",
        "        self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d)\n",
        "        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d)\n",
        "        # Apply weight normalization if enabled.\n",
        "        if weight_norm:\n",
        "            self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2)\n",
        "\n",
        "        # Define a downsampling layer if input and output dimensions differ.\n",
        "        self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None\n",
        "\n",
        "    # Define the forward pass for the block.\n",
        "    def forward(self, x):\n",
        "        # Apply dropout and ReLU activation after first convolutional layer.\n",
        "        out = self.dropout_fn(F.relu(self.conv1(x.float())))\n",
        "        # Apply dropout and ReLU activation after second convolutional layer.\n",
        "        out = self.dropout_fn(F.relu(self.conv2(out)))\n",
        "\n",
        "        # Calculate the residual connection.\n",
        "        residual = x if self.downsample is None else self.downsample(x)\n",
        "        # Return the ReLU activation of the sum of the convolutional output and residual.\n",
        "        return F.relu(out + residual[:, :, -out.shape[2]:])\n",
        "\n",
        "# Define a class for processing future residuals.\n",
        "class FutureResidual(nn.Module):\n",
        "    # Initialize the FutureResidual class with the input feature size.\n",
        "    def __init__(self, in_features):\n",
        "        super(FutureResidual, self).__init__()\n",
        "        # Define a sequential model with two linear layers and ReLU activation.\n",
        "        self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(in_features=in_features, out_features=in_features))\n",
        "\n",
        "    # Define the forward pass.\n",
        "    def forward(self, lag_x, x):\n",
        "        # Pass the input through the sequential model and get the output.\n",
        "        out = self.net(x.squeeze())\n",
        "        # Concatenate the output with the lagged input and apply ReLU activation.\n",
        "        return F.relu(torch.cat((lag_x, out), dim=2))\n"
      ],
      "metadata": {
        "id": "cvp_OXbQZcML"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DeepTCN class as a subclass of nn.Module.\n",
        "class DeepTCN(nn.Module):\n",
        "    # Initialize the DeepTCN with specified parameters.\n",
        "    def __init__(self, cov_dims=cov_dims, num_class=num_series, embedding_dim=20, dilations=[1,2,4,8,16,24,32], p=0.25, device=torch.device('cuda')):\n",
        "        super(DeepTCN, self).__init__()\n",
        "        # Set input dimensions, covariate dimensions, and device for computation.\n",
        "        self.input_dim, self.cov_dims, self.embeddings, self.device = 1+(len(cov_dims)*embedding_dim), cov_dims, [], device\n",
        "        # Create embeddings for each covariate dimension.\n",
        "        for cov in cov_dims:\n",
        "            self.embeddings.append(nn.Embedding(num_class, embedding_dim, device=device))\n",
        "\n",
        "        # Initialize the encoder as a list of residual blocks.\n",
        "        self.encoder = nn.ModuleList()\n",
        "        for d in dilations:\n",
        "            self.encoder.append(ResidualBlock(input_dim=self.input_dim, num_filters=self.input_dim, d=d))\n",
        "        # Initialize the decoder using the FutureResidual module.\n",
        "        self.decoder = FutureResidual(in_features=self.input_dim-1)\n",
        "        # Define a multi-layer perceptron (MLP) for processing the output.\n",
        "        self.mlp = nn.Sequential(nn.Linear(1158, 8), nn.BatchNorm1d(8), nn.SiLU(), nn.Dropout(p), nn.Linear(8,1), nn.ReLU())\n",
        "\n",
        "    # Define the forward pass of the model.\n",
        "    def forward(self, x, current_cov, next_cov):\n",
        "        # Initialize lists to store embeddings for current and next covariates.\n",
        "        current_cov_embeddings, next_cov_embeddings = [], []\n",
        "        # Generate embeddings for each current and next covariate.\n",
        "        for cov_idx, cov_dim in enumerate(self.cov_dims):\n",
        "            current_cov_embeddings.append(self.embeddings[cov_idx](current_cov[:,:,cov_idx].to(self.device).long()))\n",
        "            next_cov_embeddings.append(self.embeddings[cov_idx](next_cov[:,:,cov_idx].to(self.device).long()))\n",
        "        # Concatenate all current covariate embeddings.\n",
        "        embed_concat = torch.cat(current_cov_embeddings, dim=2).to(self.device)\n",
        "        # Concatenate all next covariate embeddings.\n",
        "        next_cov_concat = torch.cat(next_cov_embeddings, dim=2).to(self.device)\n",
        "\n",
        "        # Combine time series data with current covariate embeddings.\n",
        "        encoder_input = torch.cat((x.unsqueeze(2), embed_concat), dim=2)\n",
        "        # Permute the encoder input to match expected dimensions.\n",
        "        encoder_input = encoder_input.permute(0, 2, 1)\n",
        "\n",
        "        # Pass the input through each layer of the encoder.\n",
        "        for layer in self.encoder:\n",
        "            encoder_input = layer(encoder_input)\n",
        "        # Permute the encoder output and reshape it.\n",
        "        encoder_output = encoder_input.permute(0, 2, 1)\n",
        "        encoder_output = torch.reshape(encoder_output, (encoder_output.shape[0], 1, -1))\n",
        "        # Repeat encoder output to match dimensions of next covariates.\n",
        "        encoder_output = torch.repeat_interleave(encoder_output, next_cov_concat.shape[1], dim=1)\n",
        "\n",
        "        # Process the encoder output and next covariates through the decoder.\n",
        "        decoder_output = self.decoder(lag_x=encoder_output, x=next_cov_concat)\n",
        "        # Obtain dimensions for reshaping the decoder output.\n",
        "        t, n = decoder_output.size(0), decoder_output.size(1)\n",
        "        # Reshape and pass the decoder output through the MLP.\n",
        "        decoder_output = decoder_output.view(t * n, -1)\n",
        "        output = self.mlp(decoder_output.float())\n",
        "        # Reshape the final output to match the target dimensions.\n",
        "        output = output.view(t, n, -1)\n",
        "\n",
        "        # Return the squeezed output tensor.\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "fnLOPdC0aC_u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to train the model.\n",
        "def train(model, device=torch.device('cuda'), num_epochs = 1, learning_rate = 1e-3):\n",
        "    # Get the length of the training data loader.\n",
        "    train_len = len(train_loader)\n",
        "    # Initialize the optimizer with model parameters and learning rate.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Create an array to store loss values for each epoch.\n",
        "    loss_summary = np.zeros((train_len * num_epochs))\n",
        "    # Define the loss function as mean squared error.\n",
        "    loss_fn = F.mse_loss\n",
        "\n",
        "    # Iterate over the specified number of epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set the model to training mode.\n",
        "        model.train()\n",
        "        # Create an array to store loss for each batch in an epoch.\n",
        "        loss_epoch = np.zeros(len(train_loader))\n",
        "\n",
        "        # Iterate over the training data loader.\n",
        "        pbar = tqdm(train_loader)\n",
        "        for (ts_data_batch, current_covs_batch, labels_batch, next_covs_batch) in pbar:\n",
        "            # Reset gradients to zero before starting backpropagation.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize a tensor for storing loss.\n",
        "            loss = torch.zeros(1, device=device, dtype=torch.float32)\n",
        "            # Pass the data through the model and get output.\n",
        "            out = model(ts_data_batch.to(device), current_covs_batch.to(device), next_covs_batch.to(device))\n",
        "            # Calculate the loss between the output and the labels.\n",
        "            loss = loss_fn(out.float(), labels_batch.squeeze().to(device).float())\n",
        "\n",
        "            # Display the current loss in the progress bar.\n",
        "            pbar.set_description(f\"Loss:{loss.item()}\")\n",
        "            # Perform backpropagation.\n",
        "            loss.backward()\n",
        "            # Update model parameters.\n",
        "            optimizer.step()\n",
        "\n",
        "        # Store the loss for the current epoch in the loss summary.\n",
        "        loss_summary[epoch * train_len:(epoch + 1) * train_len] = loss.cpu().detach()\n",
        "\n",
        "    # Return the loss summary and optimizer for further use.\n",
        "    return loss_summary, optimizer\n",
        "\n",
        "# Define a function to evaluate the model.\n",
        "def evaluate(model, optimizer, device=torch.device('cuda')):\n",
        "    # Initialize a list to store results.\n",
        "    results = []\n",
        "\n",
        "    # Disable gradient calculations for evaluation.\n",
        "    with torch.no_grad():\n",
        "        # Set the model to evaluation mode.\n",
        "        model.eval()\n",
        "        # Initialize an array to store loss for each batch in an epoch.\n",
        "        loss_epoch = np.zeros(len(train_loader))\n",
        "\n",
        "        # Iterate over the test data loader.\n",
        "        pbar = tqdm(test_loader)\n",
        "        for (ts_data_batch, current_covs_batch, v_batch, labels_batch, next_covs_batch) in pbar:\n",
        "            # Reset gradients to zero before starting backpropagation.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Pass the data through the model and get output.\n",
        "            out = model(ts_data_batch.to(device), current_covs_batch.to(device), next_covs_batch.to(device))\n",
        "            # Append the squeezed output to the results list.\n",
        "            results.append(out.squeeze(0).cpu())\n",
        "\n",
        "    # Concatenate all results to form a single predictions tensor.\n",
        "    predictions = torch.cat(results)\n",
        "    # Define the criterion as mean squared error loss.\n",
        "    criterion = nn.MSELoss()\n",
        "    # Calculate the root mean squared error (RMSE) between predictions and labels.\n",
        "    test_rmse = torch.sqrt(criterion(predictions, labels_batch)).item()\n",
        "    # Return the calculated RMSE value.\n",
        "    return test_rmse\n"
      ],
      "metadata": {
        "id": "fD0jk7aWadSM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepTCN(device=torch.device('cuda')).cuda()\n",
        "loss, optimizer = train(model, num_epochs=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "T9MkAXmUanp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/Colab_Notebooks/ads_506/deeptcn.pth'\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "fkT8qQVEhyrj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, optimizer, device=torch.device('cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqj4R5A-asT4",
        "outputId": "7fff38d4-44b2-43c0-ab64-92a4594550b5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15276597440242767"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}